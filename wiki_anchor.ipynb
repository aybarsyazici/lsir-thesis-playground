{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3836 stakeholders before clustering...\n",
      "Anchor file found.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "data_path = Path(\"/home/yazici/playground/new-prompts/output/gpt-4o-2024-08-06_event2_newest_report_20241107-035313.json\")\n",
    "anchor_file_path = Path(\"/mnt/datasets/dop-position-mining/wiki-anchor/anchor_target_counts.csv\")\n",
    "eval_output_path = Path(\"/home/yazici/playground/new-prompts/output/eval_output.json\")\n",
    "eval_output_path_new = Path(\"/home/yazici/playground/new-prompts/output/eval_output_new.json\")\n",
    "# read eval_output.json (if it exists, otherwise create it)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "df = pd.read_json(data_path)\n",
    "df_report = df.explode(\"positions\").reset_index(drop=True)\n",
    "# Normalize the 'positions' field into a separate dataframe\n",
    "df_positions = pd.json_normalize(df_report[\"positions\"])\n",
    "# drop rows that have the targets as empty lists\n",
    "df_positions = df_positions[df_positions[\"targets\"].apply(len) > 0]\n",
    "# --- Stakeholder Clustering ---\n",
    "print(\n",
    "    f\"{len(df_positions['stakeholder'].unique())} stakeholders\"\n",
    "    \" before clustering...\"\n",
    ")\n",
    "stakeholders = df_positions[\"stakeholder\"].tolist()\n",
    "stakeholders = [stakeholder.lower().strip() for stakeholder in stakeholders]\n",
    "# does df_max_views.parquet exist?\n",
    "anchor_file_path_dir = Path(anchor_file_path).parent\n",
    "if (anchor_file_path_dir / \"df_max_views.parquet\").exists():\n",
    "    print(\"Anchor file found.\")\n",
    "    df_max_views = pd.read_parquet(anchor_file_path_dir / \"df_max_views.parquet\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Anchor file not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('hg', 'arkohut/jina-embeddings-v3'), ('st', 'all-mpnet-base-v2'), ('st', 'Alibaba-NLP/gte-multilingual-base'))\n",
      "(('hg', 'arkohut/jina-embeddings-v3'), ('st', 'all-mpnet-base-v2'), ('st', 'dunzhang/stella_en_1.5B_v5'))\n",
      "(('hg', 'arkohut/jina-embeddings-v3'), ('st', 'all-mpnet-base-v2'), ('st', 'intfloat/multilingual-e5-large-instruct'))\n",
      "(('hg', 'arkohut/jina-embeddings-v3'), ('st', 'all-mpnet-base-v2'), ('st', 'paraphrase-multilingual-mpnet-base-v2'))\n",
      "(('hg', 'arkohut/jina-embeddings-v3'), ('st', 'Alibaba-NLP/gte-multilingual-base'), ('st', 'dunzhang/stella_en_1.5B_v5'))\n",
      "(('hg', 'arkohut/jina-embeddings-v3'), ('st', 'Alibaba-NLP/gte-multilingual-base'), ('st', 'intfloat/multilingual-e5-large-instruct'))\n",
      "(('hg', 'arkohut/jina-embeddings-v3'), ('st', 'Alibaba-NLP/gte-multilingual-base'), ('st', 'paraphrase-multilingual-mpnet-base-v2'))\n",
      "(('hg', 'arkohut/jina-embeddings-v3'), ('st', 'dunzhang/stella_en_1.5B_v5'), ('st', 'intfloat/multilingual-e5-large-instruct'))\n",
      "(('hg', 'arkohut/jina-embeddings-v3'), ('st', 'dunzhang/stella_en_1.5B_v5'), ('st', 'paraphrase-multilingual-mpnet-base-v2'))\n",
      "(('hg', 'arkohut/jina-embeddings-v3'), ('st', 'intfloat/multilingual-e5-large-instruct'), ('st', 'paraphrase-multilingual-mpnet-base-v2'))\n",
      "(('st', 'all-mpnet-base-v2'), ('st', 'Alibaba-NLP/gte-multilingual-base'), ('st', 'dunzhang/stella_en_1.5B_v5'))\n",
      "(('st', 'all-mpnet-base-v2'), ('st', 'Alibaba-NLP/gte-multilingual-base'), ('st', 'intfloat/multilingual-e5-large-instruct'))\n",
      "(('st', 'all-mpnet-base-v2'), ('st', 'Alibaba-NLP/gte-multilingual-base'), ('st', 'paraphrase-multilingual-mpnet-base-v2'))\n",
      "(('st', 'all-mpnet-base-v2'), ('st', 'dunzhang/stella_en_1.5B_v5'), ('st', 'intfloat/multilingual-e5-large-instruct'))\n",
      "(('st', 'all-mpnet-base-v2'), ('st', 'dunzhang/stella_en_1.5B_v5'), ('st', 'paraphrase-multilingual-mpnet-base-v2'))\n",
      "(('st', 'all-mpnet-base-v2'), ('st', 'intfloat/multilingual-e5-large-instruct'), ('st', 'paraphrase-multilingual-mpnet-base-v2'))\n",
      "(('st', 'Alibaba-NLP/gte-multilingual-base'), ('st', 'dunzhang/stella_en_1.5B_v5'), ('st', 'intfloat/multilingual-e5-large-instruct'))\n",
      "(('st', 'Alibaba-NLP/gte-multilingual-base'), ('st', 'dunzhang/stella_en_1.5B_v5'), ('st', 'paraphrase-multilingual-mpnet-base-v2'))\n",
      "(('st', 'Alibaba-NLP/gte-multilingual-base'), ('st', 'intfloat/multilingual-e5-large-instruct'), ('st', 'paraphrase-multilingual-mpnet-base-v2'))\n",
      "(('st', 'dunzhang/stella_en_1.5B_v5'), ('st', 'intfloat/multilingual-e5-large-instruct'), ('st', 'paraphrase-multilingual-mpnet-base-v2'))\n",
      "(('hg', 'arkohut/jina-embeddings-v3'), ('st', 'all-mpnet-base-v2'), ('st', 'Alibaba-NLP/gte-multilingual-base'), ('st', 'dunzhang/stella_en_1.5B_v5'))\n",
      "(('hg', 'arkohut/jina-embeddings-v3'), ('st', 'all-mpnet-base-v2'), ('st', 'Alibaba-NLP/gte-multilingual-base'), ('st', 'intfloat/multilingual-e5-large-instruct'))\n",
      "(('hg', 'arkohut/jina-embeddings-v3'), ('st', 'all-mpnet-base-v2'), ('st', 'Alibaba-NLP/gte-multilingual-base'), ('st', 'paraphrase-multilingual-mpnet-base-v2'))\n",
      "(('hg', 'arkohut/jina-embeddings-v3'), ('st', 'all-mpnet-base-v2'), ('st', 'dunzhang/stella_en_1.5B_v5'), ('st', 'intfloat/multilingual-e5-large-instruct'))\n",
      "(('hg', 'arkohut/jina-embeddings-v3'), ('st', 'all-mpnet-base-v2'), ('st', 'dunzhang/stella_en_1.5B_v5'), ('st', 'paraphrase-multilingual-mpnet-base-v2'))\n",
      "(('hg', 'arkohut/jina-embeddings-v3'), ('st', 'all-mpnet-base-v2'), ('st', 'intfloat/multilingual-e5-large-instruct'), ('st', 'paraphrase-multilingual-mpnet-base-v2'))\n",
      "(('hg', 'arkohut/jina-embeddings-v3'), ('st', 'Alibaba-NLP/gte-multilingual-base'), ('st', 'dunzhang/stella_en_1.5B_v5'), ('st', 'intfloat/multilingual-e5-large-instruct'))\n",
      "(('hg', 'arkohut/jina-embeddings-v3'), ('st', 'Alibaba-NLP/gte-multilingual-base'), ('st', 'dunzhang/stella_en_1.5B_v5'), ('st', 'paraphrase-multilingual-mpnet-base-v2'))\n",
      "(('hg', 'arkohut/jina-embeddings-v3'), ('st', 'Alibaba-NLP/gte-multilingual-base'), ('st', 'intfloat/multilingual-e5-large-instruct'), ('st', 'paraphrase-multilingual-mpnet-base-v2'))\n",
      "(('hg', 'arkohut/jina-embeddings-v3'), ('st', 'dunzhang/stella_en_1.5B_v5'), ('st', 'intfloat/multilingual-e5-large-instruct'), ('st', 'paraphrase-multilingual-mpnet-base-v2'))\n",
      "(('st', 'all-mpnet-base-v2'), ('st', 'Alibaba-NLP/gte-multilingual-base'), ('st', 'dunzhang/stella_en_1.5B_v5'), ('st', 'intfloat/multilingual-e5-large-instruct'))\n",
      "(('st', 'all-mpnet-base-v2'), ('st', 'Alibaba-NLP/gte-multilingual-base'), ('st', 'dunzhang/stella_en_1.5B_v5'), ('st', 'paraphrase-multilingual-mpnet-base-v2'))\n",
      "(('st', 'all-mpnet-base-v2'), ('st', 'Alibaba-NLP/gte-multilingual-base'), ('st', 'intfloat/multilingual-e5-large-instruct'), ('st', 'paraphrase-multilingual-mpnet-base-v2'))\n",
      "(('st', 'all-mpnet-base-v2'), ('st', 'dunzhang/stella_en_1.5B_v5'), ('st', 'intfloat/multilingual-e5-large-instruct'), ('st', 'paraphrase-multilingual-mpnet-base-v2'))\n",
      "(('st', 'Alibaba-NLP/gte-multilingual-base'), ('st', 'dunzhang/stella_en_1.5B_v5'), ('st', 'intfloat/multilingual-e5-large-instruct'), ('st', 'paraphrase-multilingual-mpnet-base-v2'))\n",
      "(('hg', 'arkohut/jina-embeddings-v3'), ('st', 'all-mpnet-base-v2'), ('st', 'Alibaba-NLP/gte-multilingual-base'), ('st', 'dunzhang/stella_en_1.5B_v5'), ('st', 'intfloat/multilingual-e5-large-instruct'))\n",
      "(('hg', 'arkohut/jina-embeddings-v3'), ('st', 'all-mpnet-base-v2'), ('st', 'Alibaba-NLP/gte-multilingual-base'), ('st', 'dunzhang/stella_en_1.5B_v5'), ('st', 'paraphrase-multilingual-mpnet-base-v2'))\n",
      "(('hg', 'arkohut/jina-embeddings-v3'), ('st', 'all-mpnet-base-v2'), ('st', 'Alibaba-NLP/gte-multilingual-base'), ('st', 'intfloat/multilingual-e5-large-instruct'), ('st', 'paraphrase-multilingual-mpnet-base-v2'))\n",
      "(('hg', 'arkohut/jina-embeddings-v3'), ('st', 'all-mpnet-base-v2'), ('st', 'dunzhang/stella_en_1.5B_v5'), ('st', 'intfloat/multilingual-e5-large-instruct'), ('st', 'paraphrase-multilingual-mpnet-base-v2'))\n",
      "(('hg', 'arkohut/jina-embeddings-v3'), ('st', 'Alibaba-NLP/gte-multilingual-base'), ('st', 'dunzhang/stella_en_1.5B_v5'), ('st', 'intfloat/multilingual-e5-large-instruct'), ('st', 'paraphrase-multilingual-mpnet-base-v2'))\n",
      "(('st', 'all-mpnet-base-v2'), ('st', 'Alibaba-NLP/gte-multilingual-base'), ('st', 'dunzhang/stella_en_1.5B_v5'), ('st', 'intfloat/multilingual-e5-large-instruct'), ('st', 'paraphrase-multilingual-mpnet-base-v2'))\n",
      "(('hg', 'arkohut/jina-embeddings-v3'), ('st', 'all-mpnet-base-v2'), ('st', 'Alibaba-NLP/gte-multilingual-base'), ('st', 'dunzhang/stella_en_1.5B_v5'), ('st', 'intfloat/multilingual-e5-large-instruct'), ('st', 'paraphrase-multilingual-mpnet-base-v2'))\n",
      "Total number of combinations: 42\n"
     ]
    }
   ],
   "source": [
    "# create all possible combinations of the following items:\n",
    "# 1. (\"hg\", \"arkohut/jina-embeddings-v3\")\n",
    "# 2. (\"st\", \"all-mpnet-base-v2\")\n",
    "# 3. (\"hg\", \"Alibaba-NLP/gte-multilingual-base\")\n",
    "# 4. (\"hg\", \"dunzhang/stella_en_1.5B_v5\"),\n",
    "# 5. (\"hg\", \"intfloat/multilingual-e5-large-instruct\"),\n",
    "# 6. (\"st\", \"paraphrase-multilingual-mpnet-base-v2\")\n",
    "\n",
    "import itertools\n",
    "\n",
    "# Define the list of items\n",
    "items = [\n",
    "    (\"hg\", \"arkohut/jina-embeddings-v3\"),\n",
    "    (\"st\", \"all-mpnet-base-v2\"),\n",
    "    (\"st\", \"Alibaba-NLP/gte-multilingual-base\"),\n",
    "    (\"st\", \"dunzhang/stella_en_1.5B_v5\"),\n",
    "    (\"st\", \"intfloat/multilingual-e5-large-instruct\"),\n",
    "    (\"st\", \"paraphrase-multilingual-mpnet-base-v2\")\n",
    "]\n",
    "\n",
    "# Generate all combinations of sizes 1 to 6\n",
    "all_combinations = []\n",
    "for r in range(3, len(items) + 1):\n",
    "    combinations = itertools.combinations(items, r)\n",
    "    all_combinations.extend(combinations)\n",
    "\n",
    "# Print the combinations\n",
    "for combination in all_combinations:\n",
    "    print(combination)\n",
    "\n",
    "# If you want the total count of combinations\n",
    "print(f\"Total number of combinations: {len(all_combinations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_param_combinations(params):\n",
    "    keys, values = zip(*params.items())\n",
    "    combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "    return combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of combinations: 756\n"
     ]
    }
   ],
   "source": [
    "search_grid = {\n",
    "    \"threshold\": [0.05, 0.1, 0.15, 0.2, 0.25, 0.3],\n",
    "    \"model_names\": all_combinations,\n",
    "    \"voting\": [\"all\", \"majority\", \"any\"]\n",
    "}\n",
    "\n",
    "combinations = get_param_combinations(search_grid)\n",
    "print(f\"Total number of combinations: {len(combinations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'threshold': 0.1,\n",
       " 'model_names': (('st', 'all-mpnet-base-v2'),\n",
       "  ('st', 'Alibaba-NLP/gte-multilingual-base'),\n",
       "  ('st', 'dunzhang/stella_en_1.5B_v5'),\n",
       "  ('st', 'intfloat/multilingual-e5-large-instruct')),\n",
       " 'voting': 'majority'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinations[217]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    (\"st\", \"arkohut/jina-embeddings-v3\", \"cuda:0\"), (\"st\", \"all-mpnet-base-v2\", \"cuda:1\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yazici/.cache/pypoetry/virtualenvs/new-prompts-cm8HmoVy-py3.9/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(('st', 'all-mpnet-base-v2'),\n",
       " ('st', 'Alibaba-NLP/gte-multilingual-base'),\n",
       " ('st', 'dunzhang/stella_en_1.5B_v5'),\n",
       " ('st', 'intfloat/multilingual-e5-large-instruct'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import post_processing_multimodel\n",
    "\n",
    "model_names = combinations[217][\"model_names\"]\n",
    "threshold = combinations[217][\"threshold\"]\n",
    "model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Alibaba-NLP/gte-multilingual-base were not used when initializing NewModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "models = post_processing_multimodel.get_models(model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeds for model: ('st', 'all-mpnet-base-v2')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2168686/2400632645.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  df_embeddings.append(torch.load(post_processing_multimodel.device_to_embed_map[model_name[1]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeds for model: ('st', 'Alibaba-NLP/gte-multilingual-base')\n",
      "Loading embeds for model: ('st', 'dunzhang/stella_en_1.5B_v5')\n",
      "Loading embeds for model: ('st', 'intfloat/multilingual-e5-large-instruct')\n"
     ]
    }
   ],
   "source": [
    "df_embeddings = []\n",
    "for model_name in model_names:\n",
    "    print(f\"Loading embeds for model: {model_name}\")\n",
    "    df_embeddings.append(torch.load(post_processing_multimodel.device_to_embed_map[model_name[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'post_processing_multimodel' from '/home/yazici/playground/new-prompts/post_processing_multimodel.py'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(post_processing_multimodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: Qwen2Model \n",
       "  (1): Pooling({'word_embedding_dimension': 1536, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Dense({'in_features': 1536, 'out_features': 1024, 'bias': True, 'activation_function': 'torch.nn.modules.linear.Identity'})\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_file_path = Path(\"/home/yazici/playground/new-prompts/temp-files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In else | Encoding stakeholders using sentence-transformers/all-mpnet-base-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b2a81c768f94b52b684b4d63becf4fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In else | Encoding stakeholders using Alibaba-NLP/gte-multilingual-base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61616a480d774562b102c71e4a0a811c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In If | Encoding stakeholders using dunzhang/stella_en_1.5B_v5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d38310bf21084084973804c4d874cc7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In Elif | Encoding stakeholders using intfloat/multilingual-e5-large-instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98ce323eaf534b2eb1badcf269943530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query chunks: 100%|██████████| 39/39 [02:37<00:00,  4.04s/it]\n",
      "Query chunks: 100%|██████████| 39/39 [02:53<00:00,  4.44s/it]\n",
      "Query chunks: 100%|██████████| 39/39 [04:26<00:00,  6.82s/it]\n",
      "Query chunks: 100%|██████████| 39/39 [04:13<00:00,  6.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit count: 17 | Hit percentage before wiki: 0.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching wiki info: 100%|██████████| 7/7 [00:01<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of wiki corpus: 207\n",
      "Length of missing stakeholders: 3811\n",
      "\n",
      "In else | Encoding stakeholders using sentence-transformers/all-mpnet-base-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "becd0ffcf7fd463895de5b59e2e3ccf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In else | Encoding stakeholders using Alibaba-NLP/gte-multilingual-base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "665f907c01dc4803946a174fe6ca4479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In If | Encoding stakeholders using dunzhang/stella_en_1.5B_v5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c858ba80018b4f07bab29a7971bd0d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In Elif | Encoding stakeholders using intfloat/multilingual-e5-large-instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddc9529d666b42ce9d69901bbdacd06f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In else | Encoding stakeholders using sentence-transformers/all-mpnet-base-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39e987120f54441a9ec6881b44b23f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In else | Encoding stakeholders using Alibaba-NLP/gte-multilingual-base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a4ab1ab22c744cf871b11ae4650e1ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In If | Encoding stakeholders using dunzhang/stella_en_1.5B_v5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "123a61aed0704a48880df15aa8017abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In Elif | Encoding stakeholders using intfloat/multilingual-e5-large-instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4aec4106749470a93e3dafe5f3bc2a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query chunks: 100%|██████████| 39/39 [00:00<00:00, 687.74it/s]\n",
      "Query chunks: 100%|██████████| 39/39 [00:00<00:00, 1074.59it/s]\n",
      "Query chunks: 100%|██████████| 39/39 [00:00<00:00, 787.42it/s]\n",
      "Query chunks: 100%|██████████| 39/39 [00:00<00:00, 1182.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit count: 41 | Hit percentage after wiki: 0.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting stakeholders: 100%|██████████| 3819/3819 [00:00<00:00, 6772.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In else | Encoding stakeholders using sentence-transformers/all-mpnet-base-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60737d30b62643ed9ed9bf44f8a3e59a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In else | Encoding stakeholders using Alibaba-NLP/gte-multilingual-base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aa778d73ea1409ca70fcd579d64dfe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In If | Encoding stakeholders using dunzhang/stella_en_1.5B_v5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a4590f27e947f9b17603f3a4072b0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In Elif | Encoding stakeholders using intfloat/multilingual-e5-large-instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9f47e9fd7b242b9a19f1c322392ce71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering stakeholders/targets threshold 0.1...\n",
      "Fast clustering start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding clusters: 100%|██████████| 4/4 [00:00<00:00, 10.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering done after 0.40 sec\n",
      "Hit count: 41 | Hit percentage after clustering: 0.29%\n"
     ]
    }
   ],
   "source": [
    "new_stakeholders, stakeholder_index_to_wiki_id, stakeholder_replacement = post_processing_multimodel.wiki_anchor(\n",
    "    stakeholders=stakeholders,\n",
    "    df_embeddings=df_embeddings,\n",
    "    device=\"cuda\",\n",
    "    df_max_views=df_max_views,\n",
    "    models=models,\n",
    "    voting=combinations[217][\"voting\"],\n",
    "    output_dir=temp_file_path,\n",
    "    threshold=threshold,\n",
    "    clustering_method=\"fast\",\n",
    "    event_name=\"event2_multimodel\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# read all_results_eventname.json\n",
    "with open(temp_file_path / \"all_results_event2_multimodel.pkl\", \"rb\") as f:\n",
    "    all_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('st', 'all-mpnet-base-v2'),\n",
       " ('st', 'Alibaba-NLP/gte-multilingual-base'),\n",
       " ('st', 'dunzhang/stella_en_1.5B_v5'),\n",
       " ('st', 'intfloat/multilingual-e5-large-instruct'))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9107993245124817,\n",
       "  'corpus_id': 4671886,\n",
       "  'anchor_text': 'zelensky',\n",
       "  'model_idx': 0},\n",
       " {'score': 0.8802839517593384,\n",
       "  'corpus_id': 4671876,\n",
       "  'anchor_text': 'zelenodolsky',\n",
       "  'model_idx': 0},\n",
       " {'score': 0.8370147347450256,\n",
       "  'corpus_id': 2479776,\n",
       "  'anchor_text': 'lev zeleny',\n",
       "  'model_idx': 0},\n",
       " {'score': 0.8345829248428345,\n",
       "  'corpus_id': 4671855,\n",
       "  'anchor_text': 'zelenchukskaya',\n",
       "  'model_idx': 0},\n",
       " {'score': 0.8317021131515503,\n",
       "  'corpus_id': 4667176,\n",
       "  'anchor_text': 'zalischyky',\n",
       "  'model_idx': 0},\n",
       " {'score': 0.9817342162132263,\n",
       "  'corpus_id': 4671886,\n",
       "  'anchor_text': 'zelensky',\n",
       "  'model_idx': 1},\n",
       " {'score': 0.9017746448516846,\n",
       "  'corpus_id': 4671852,\n",
       "  'anchor_text': 'zelenay',\n",
       "  'model_idx': 1},\n",
       " {'score': 0.8885848522186279,\n",
       "  'corpus_id': 4671853,\n",
       "  'anchor_text': 'zelenaši',\n",
       "  'model_idx': 1},\n",
       " {'score': 0.8780649304389954,\n",
       "  'corpus_id': 4470569,\n",
       "  'anchor_text': 'vladimir zelensky',\n",
       "  'model_idx': 1},\n",
       " {'score': 0.8705978393554688,\n",
       "  'corpus_id': 4671870,\n",
       "  'anchor_text': 'zelenka',\n",
       "  'model_idx': 1},\n",
       " {'score': 0.7860783338546753,\n",
       "  'corpus_id': 4671886,\n",
       "  'anchor_text': 'zelensky',\n",
       "  'model_idx': 2},\n",
       " {'score': 0.7781102657318115,\n",
       "  'corpus_id': 4475400,\n",
       "  'anchor_text': 'volodymyr zelenskyi',\n",
       "  'model_idx': 2},\n",
       " {'score': 0.7759053111076355,\n",
       "  'corpus_id': 4475398,\n",
       "  'anchor_text': 'volodymyr zelenskiy',\n",
       "  'model_idx': 2},\n",
       " {'score': 0.7688626646995544,\n",
       "  'corpus_id': 4475399,\n",
       "  'anchor_text': 'volodymyr zelensky',\n",
       "  'model_idx': 2},\n",
       " {'score': 0.7646397948265076,\n",
       "  'corpus_id': 264193,\n",
       "  'anchor_text': 'aleksey zelensky',\n",
       "  'model_idx': 2},\n",
       " {'score': 0.890964925289154,\n",
       "  'corpus_id': 264193,\n",
       "  'anchor_text': 'aleksey zelensky',\n",
       "  'model_idx': 3},\n",
       " {'score': 0.8900272846221924,\n",
       "  'corpus_id': 4671886,\n",
       "  'anchor_text': 'zelensky',\n",
       "  'model_idx': 3},\n",
       " {'score': 0.8890353441238403,\n",
       "  'corpus_id': 4475400,\n",
       "  'anchor_text': 'volodymyr zelenskyi',\n",
       "  'model_idx': 3},\n",
       " {'score': 0.8875953555107117,\n",
       "  'corpus_id': 4475399,\n",
       "  'anchor_text': 'volodymyr zelensky',\n",
       "  'model_idx': 3},\n",
       " {'score': 0.8854696750640869,\n",
       "  'corpus_id': 4470569,\n",
       "  'anchor_text': 'vladimir zelensky',\n",
       "  'model_idx': 3}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results[\"zelenskyy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive samples: 398\n",
      "Number of negative samples: 41797\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "stakeholder_eval_set_answers = \"\"\n",
    "with open(\"stakeholder_eval_set_answers.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        stakeholder_eval_set_answers += line.strip()\n",
    "\n",
    "stakeholder_eval_set_answers = json.loads(stakeholder_eval_set_answers)\n",
    "\n",
    "# Positive samples (pairs of items within the same list)\n",
    "positive_samples = []\n",
    "for sublist in stakeholder_eval_set_answers:\n",
    "    for i in range(len(sublist)):\n",
    "        for j in range(i + 1, len(sublist)):\n",
    "            positive_samples.append((sublist[i], sublist[j]))\n",
    "\n",
    "# Negative samples (pairs of items from different sublists)\n",
    "negative_samples = []\n",
    "for i in range(len(stakeholder_eval_set_answers)):\n",
    "    for j in range(i + 1, len(stakeholder_eval_set_answers)):\n",
    "        # Create all possible pairs between sublist[i] and sublist[j]\n",
    "        for element1 in stakeholder_eval_set_answers[i]:\n",
    "            for element2 in stakeholder_eval_set_answers[j]:\n",
    "                negative_samples.append((element1, element2))\n",
    "\n",
    "print(f\"Number of positive samples: {len(positive_samples)}\")\n",
    "print(f\"Number of negative samples: {len(negative_samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives (TP): 153\n",
      "False Negatives (FN): 245\n",
      "False Positives (FP): 6\n",
      "True Negatives (TN): 41515\n",
      "Precision: 0.9623\n",
      "Recall: 0.3844\n",
      "F1 Score: 0.5494\n",
      "Accuracy: 0.9940\n",
      "False Positive Rate (FPR): 0.0001\n",
      "Specificity: 0.9999\n"
     ]
    }
   ],
   "source": [
    "stakeholder_replacement_grouped = defaultdict(list)\n",
    "\n",
    "for k,v in stakeholder_replacement.items():\n",
    "    stakeholder_replacement_grouped[v].append(k)\n",
    "\n",
    "stakeholder_clusters_final = [\n",
    "    [stakeholder for stakeholder in cluster] for cluster in stakeholder_replacement_grouped.values()\n",
    "]\n",
    "\n",
    "positive_results = []\n",
    "for sublist in stakeholder_clusters_final:\n",
    "    for i in range(len(sublist)):\n",
    "        for j in range(i + 1, len(sublist)):\n",
    "            positive_results.append((sublist[i], sublist[j]))\n",
    "\n",
    "positive_samples_set = set(positive_samples)\n",
    "negative_samples_set = set(negative_samples)\n",
    "\n",
    "# Convert the samples to sorted tuples (to handle unordered pairs)\n",
    "positive_samples_set = {tuple(sorted(sample)) for sample in positive_samples}\n",
    "negative_samples_set = {tuple(sorted(sample)) for sample in negative_samples}\n",
    "\n",
    "# Calculate true positives (TP): positive samples that exist in positive_results\n",
    "true_positives_results = [sample for sample in positive_samples_set if tuple(sorted(sample)) in positive_results]\n",
    "\n",
    "# Calculate false negatives (FN): positive samples that do not exist in positive_results\n",
    "false_negatives_results = [sample for sample in positive_samples_set if tuple(sorted(sample)) not in positive_results]\n",
    "\n",
    "# Calculate false positives (FP): negative samples that exist in positive_results\n",
    "false_positives_results = [sample for sample in negative_samples_set if tuple(sorted(sample)) in positive_results]\n",
    "\n",
    "# Calculate true negatives (TN): negative samples that do not exist in positive_results\n",
    "true_negatives_results = [sample for sample in negative_samples_set if tuple(sorted(sample)) not in positive_results]\n",
    "\n",
    "\n",
    "true_positives = len(true_positives_results)\n",
    "false_negatives = len(false_negatives_results)\n",
    "false_positives = len(false_positives_results)\n",
    "true_negatives = len(true_negatives_results)\n",
    "\n",
    "# Output the results\n",
    "print(\"True Positives (TP):\", true_positives)\n",
    "print(\"False Negatives (FN):\", false_negatives)\n",
    "print(\"False Positives (FP):\", false_positives)\n",
    "print(\"True Negatives (TN):\", true_negatives)\n",
    "\n",
    "# Calculate the metrics based on TP, FP, TN, FN\n",
    "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) != 0 else 0\n",
    "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) != 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives)\n",
    "fpr = false_positives / (false_positives + true_negatives) if (false_positives + true_negatives) != 0 else 0\n",
    "specificity = true_negatives / (true_negatives + false_positives) if (true_negatives + false_positives) != 0 else 0\n",
    "\n",
    "# Output the results\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"False Positive Rate (FPR): {fpr:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('armed forces of ukraine', \"ukraine's defense forces\"),\n",
       " (\"ukraine's armed forces\", \"ukraine's defense forces\"),\n",
       " ('ukraine military', \"ukraine's defense forces\"),\n",
       " (\"ukraine's defense forces\", 'ukrainian troops'),\n",
       " ('defense forces of ukraine', \"ukraine's defense forces\"),\n",
       " ('alexei shevtsov', 'dmitry shevtsov')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_positives_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-prompts-cm8HmoVy-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
